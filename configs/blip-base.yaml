# Model Configuration
model:
  id: "Salesforce/blip-image-captioning-base"
  # For GPU: "cuda", for CPU: "cpu"
  device: "cuda"
  # Use float16 for faster inference and less memory on GPU
  torch_dtype: "float16"

# Data Paths
paths:
  image_folder: "data/val2017"
  # Timestamps will be prepended to output folders
  output_dir: "outputs"

# Generation Parameters
generation:
  # A general prompt to guide the model. Set to null for unconditional generation.
  prompt: "a photography of"
  num_captions_per_image: 3
  max_new_tokens: 75
  min_length: 20
  do_sample: true
  temperature: 0.7
  top_k: 50
  top_p: 0.95
  repetition_penalty: 1.2

# Experiment Tracking
tracking:
  enable_wandb: true
  wandb_project: "Image Captioning Technical Task"
  wandb_entity: null # Your W&B username or team, or null

# Evaluation - Requires a ground truth JSON file
evaluation:
  # Path to a JSON file with ground truth captions.
  # Example format: {"image1.jpg": ["a photo of a cat", "a cat is sleeping"], ...}
  # Set to null if you don't have reference captions.
  reference_captions_path: data/captions_val2017.json